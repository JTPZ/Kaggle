{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - Cdiscount \n",
    "\n",
    "This notebook contains a generator class for Keras called BSONIterator that can read directly from the BSON data. You can use it in combination with ImageDataGenerator for doing data augmentation.\n",
    "Source: https://www.kaggle.com/humananalog/keras-generator-for-reading-directly-from-bson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import bson\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.0.8', '1.2.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__,tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider 2 configs:\n",
    "    - mode = 'test'    : submission file merged with training so we test the model usually on a small sample\n",
    "    - mode = 'submit ' : submission file ready to send to Kaggle\n",
    "    \n",
    "Actually no need to trick the submission file as we can use model.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = 'testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'C:/Users/Juju/Documents/Cdiscount/'\n",
    "train_bson_path = os.path.join(data_dir, \"train.bson\")\n",
    "num_train_products = 7069896\n",
    "test_bson_path = os.path.join(data_dir, \"test.bson\")\n",
    "num_test_products = 1768182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Create a random train/validation split\n",
    "\n",
    "We split on products, not on individual images. Since some of the categories only have a few products, we do the split separately for each category. This creates two new tables, one for the training images and one for the validation images. There is a row for every single image, so if a product has more than one image it occurs more than once in the table. We have the possiblity to either drop a percentage or filter the top categories.\n",
    "(Note: if `drop_percentage` > 0, the progress bar doesn't go all the way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_val_set(df, split_percentage=0.2, drop_percentage=0.,num_top_categories=0):\n",
    "    # Find the product_ids for each category.\n",
    "    category_dict = defaultdict(list)\n",
    "    \n",
    "    # Add a filter before building the category_dict\n",
    "    if num_top_categories > 0:\n",
    "        print(\"We filter the top \",num_top_categories,\" categories...\")\n",
    "        top_categories_df = pd.read_csv(data_dir+\"top_categories.csv\")\n",
    "        top_categories = top_categories_df[\"category_id\"][:num_top_categories].values\n",
    "        #print(top_categories)\n",
    "    \n",
    "        for ir in tqdm(df.itertuples()):\n",
    "            if ir[4] in top_categories:\n",
    "                category_dict[ir[4]].append(ir[0])\n",
    "    else:\n",
    "        for ir in tqdm(df.itertuples()):\n",
    "            category_dict[ir[4]].append(ir[0])\n",
    "        \n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    with tqdm(total=len(df)) as pbar:\n",
    "        for category_id, product_ids in category_dict.items():\n",
    "            category_idx = cat2idx[category_id]\n",
    "\n",
    "            # Randomly remove products to make the dataset smaller.\n",
    "            keep_size = int(len(product_ids) * (1. - drop_percentage))\n",
    "            if keep_size < len(product_ids):\n",
    "                product_ids = np.random.choice(product_ids, keep_size, replace=False)\n",
    "\n",
    "            # Randomly choose the products that become part of the validation set.\n",
    "            val_size = int(len(product_ids) * split_percentage)\n",
    "            if val_size > 0:\n",
    "                val_ids = np.random.choice(product_ids, val_size, replace=False)\n",
    "            else:\n",
    "                val_ids = []\n",
    "\n",
    "            # Create a new row for each image.\n",
    "            for product_id in product_ids:\n",
    "                row = [product_id, category_idx]\n",
    "                for img_idx in range(df.loc[product_id, \"num_imgs\"]):\n",
    "                    if product_id in val_ids:\n",
    "                        val_list.append(row + [img_idx])\n",
    "                    else:\n",
    "                        train_list.append(row + [img_idx])\n",
    "                pbar.update()\n",
    "                \n",
    "    columns = [\"product_id\", \"category_idx\", \"img_idx\"]\n",
    "    train_df = pd.DataFrame(train_list, columns=columns)\n",
    "    val_df = pd.DataFrame(val_list, columns=columns)   \n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             category_idx\n",
      "category_id              \n",
      "1000021794              0\n",
      "1000012764              1\n",
      "1000012776              2\n",
      "1000012768              3\n",
      "1000012755              4\n"
     ]
    }
   ],
   "source": [
    "def make_category_tables():\n",
    "    cat2idx = {}\n",
    "    idx2cat = {}\n",
    "    for ir in categories_df.itertuples():\n",
    "        category_id = ir[0]\n",
    "        category_idx = ir[1] # iinstead of ir[4] because we changed categories_df\n",
    "        cat2idx[category_id] = category_idx\n",
    "        idx2cat[category_idx] = category_id\n",
    "    return cat2idx, idx2cat\n",
    "\n",
    "categories_df = pd.read_csv(data_dir+\"categories.csv\", index_col=0)\n",
    "print(categories_df.head())\n",
    "cat2idx, idx2cat = make_category_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juju\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\arraysetops.py:395: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We filter the top  1000  categories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7069896it [00:33, 210092.42it/s]\n",
      "  4%|█▎                            | 303259/7069896 [00:10<03:58, 28331.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train_offsets_df = pd.read_csv(data_dir+\"train_offsets.csv\", index_col=0)\n",
    "train_images_df, val_images_df = make_val_set(train_offsets_df, split_percentage=0.2,\n",
    "                                              drop_percentage=0.95,num_top_categories=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_idx</th>\n",
       "      <th>img_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17848689</td>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6240512</td>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8800088</td>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4374649</td>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5694737</td>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  category_idx  img_idx\n",
       "0    17848689           619        0\n",
       "1     6240512           619        0\n",
       "2     8800088           619        0\n",
       "3     4374649           619        0\n",
       "4     5694737           619        0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_idx</th>\n",
       "      <th>img_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4451510</td>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4451510</td>\n",
       "      <td>619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4451510</td>\n",
       "      <td>619</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4451510</td>\n",
       "      <td>619</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18556624</td>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  category_idx  img_idx\n",
       "0     4451510           619        0\n",
       "1     4451510           619        1\n",
       "2     4451510           619        2\n",
       "3     4451510           619        3\n",
       "4    18556624           619        0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 420654\n",
      "Number of validation images: 104257\n",
      "Total images: 524911\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training images:\", len(train_images_df))\n",
    "print(\"Number of validation images:\", len(val_images_df))\n",
    "print(\"Total images:\", len(train_images_df) + len(val_images_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images_df[\"category_idx\"].unique()), len(val_images_df[\"category_idx\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the lookup tables as CSV so that we don't need to repeat the above procedure again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images_df.to_csv(\"train_images.csv\")\n",
    "val_images_df.to_csv(\"val_images.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - The generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the lookup tables from the CSV files (you don't need to do this if you just did all the steps from part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juju\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\arraysetops.py:395: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_offsets_df = pd.read_csv(data_dir+\"train_offsets.csv\", index_col=0)\n",
    "train_images_df = pd.read_csv(data_dir+\"train_images.csv\", index_col=0)\n",
    "val_images_df = pd.read_csv(data_dir+\"val_images.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras generator is implemented by the `BSONIterator` class. It creates batches of images (and their one-hot encoded labels) directly from the BSON file. It can be used with multiple workers.\n",
    "\n",
    "**Note:** For fastest results, put the train.bson and test.bson files on a fast drive (SSD).\n",
    "\n",
    "See also the code in: https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import Iterator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "\n",
    "class BSONIterator(Iterator):\n",
    "    def __init__(self, bson_file, images_df, offsets_df, num_class,\n",
    "                 image_data_generator, lock, target_size=(180, 180), \n",
    "                 with_labels=True, batch_size=32, shuffle=False, seed=None):\n",
    "\n",
    "        self.file = bson_file\n",
    "        self.images_df = images_df\n",
    "        self.offsets_df = offsets_df\n",
    "        self.with_labels = with_labels\n",
    "        self.samples = len(images_df)\n",
    "        self.num_class = num_class\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.target_size = tuple(target_size)\n",
    "        self.image_shape = self.target_size + (3,)\n",
    "\n",
    "        print(\"Found %d images belonging to %d classes.\" % (self.samples, self.num_class))\n",
    "\n",
    "        super(BSONIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n",
    "        self.lock = lock\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n",
    "        if self.with_labels:\n",
    "            batch_y = np.zeros((len(batch_x), self.num_class), dtype=K.floatx())\n",
    "\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            # Protect file and dataframe access with a lock.\n",
    "            with self.lock:\n",
    "                image_row = self.images_df.iloc[j]\n",
    "                product_id = image_row[\"product_id\"]\n",
    "                offset_row = self.offsets_df.loc[product_id]\n",
    "\n",
    "                # Read this product's data from the BSON file.\n",
    "                self.file.seek(offset_row[\"offset\"])\n",
    "                item_data = self.file.read(offset_row[\"length\"])\n",
    "\n",
    "            # Grab the image from the product.\n",
    "            item = bson.BSON.decode(item_data)\n",
    "            img_idx = image_row[\"img_idx\"]\n",
    "            bson_img = item[\"imgs\"][img_idx][\"picture\"]\n",
    "\n",
    "            # Load the image.\n",
    "            img = load_img(io.BytesIO(bson_img), target_size=self.target_size)\n",
    "\n",
    "            # Preprocess the image.\n",
    "            x = img_to_array(img)\n",
    "            x = self.image_data_generator.random_transform(x)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "\n",
    "            # Add the image and the label to the batch (one-hot encoded).\n",
    "            batch_x[i] = x\n",
    "            if self.with_labels:\n",
    "                batch_y[i, image_row[\"category_idx\"]] = 1\n",
    "\n",
    "        if self.with_labels:\n",
    "            return batch_x, batch_y\n",
    "        else:\n",
    "            return batch_x\n",
    "        \n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)[0]\n",
    "        return self._get_batches_of_transformed_samples(index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bson_file = open(train_bson_path, \"rb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the training and validation generators read from the same BSON file, they need to use the same lock to protect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generator for training and a generator for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 420654 images belonging to 5270 classes.\n",
      "Found 104257 images belonging to 5270 classes.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 5270 #How is this used? How can i reduce it to the effective number of classes?\n",
    "num_train_images = len(train_images_df)\n",
    "num_val_images = len(val_images_df)\n",
    "batch_size = 256 #128\n",
    "\n",
    "# Tip: use ImageDataGenerator for data augmentation and preprocessing.\n",
    "train_datagen = ImageDataGenerator()\n",
    "train_gen = BSONIterator(train_bson_file, train_images_df, train_offsets_df, \n",
    "                         num_classes, train_datagen, lock,\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "val_gen = BSONIterator(train_bson_file, val_images_df, train_offsets_df,\n",
    "                       num_classes, val_datagen, lock,\n",
    "                       batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training\n",
    "\n",
    "Create a very simple Keras model and train it, to test that the generators work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define the model\n",
    "The model is ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 180, 180, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 90, 90, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 90, 90, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 45, 45, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 45, 45, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 22, 22, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5270)              679830    \n",
      "=================================================================\n",
      "Total params: 773,078\n",
      "Trainable params: 773,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\", input_shape=(180, 180, 3)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(128, 3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train the model\n",
    "The propagation of all data (forward and backward) is called one __epoch__.<br>\n",
    "So the number of steps per epoch is:\n",
    "$\\frac{num\\_images}{batch\\_size}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 194/1643 [==>...........................] - ETA: 112592s - loss: 15.4136 - acc: 0.0000e+0 - ETA: 78648s - loss: 14.9717 - acc: 0.0020    - ETA: 73873s - loss: 14.3964 - acc: 0.00 - ETA: 66179s - loss: 13.5975 - acc: 0.00 - ETA: 61312s - loss: 12.7902 - acc: 0.00 - ETA: 58038s - loss: 12.0652 - acc: 0.00 - ETA: 55804s - loss: 11.4964 - acc: 0.00 - ETA: 54113s - loss: 11.0525 - acc: 0.00 - ETA: 52854s - loss: 10.6850 - acc: 0.00 - ETA: 51754s - loss: 10.3572 - acc: 0.00 - ETA: 50982s - loss: 10.0903 - acc: 0.00 - ETA: 50479s - loss: 9.8481 - acc: 0.0075 - ETA: 49869s - loss: 9.6467 - acc: 0.007 - ETA: 49521s - loss: 9.4590 - acc: 0.007 - ETA: 49133s - loss: 9.2955 - acc: 0.006 - ETA: 48868s - loss: 9.1363 - acc: 0.006 - ETA: 48413s - loss: 8.9902 - acc: 0.006 - ETA: 48006s - loss: 8.8686 - acc: 0.007 - ETA: 47696s - loss: 8.7468 - acc: 0.007 - ETA: 47468s - loss: 8.6355 - acc: 0.008 - ETA: 47260s - loss: 8.5369 - acc: 0.008 - ETA: 46979s - loss: 8.4457 - acc: 0.008 - ETA: 46755s - loss: 8.3627 - acc: 0.008 - ETA: 46536s - loss: 8.2905 - acc: 0.009 - ETA: 46312s - loss: 8.2172 - acc: 0.008 - ETA: 46123s - loss: 8.1482 - acc: 0.008 - ETA: 45946s - loss: 8.0881 - acc: 0.009 - ETA: 45759s - loss: 8.0287 - acc: 0.009 - ETA: 45619s - loss: 7.9697 - acc: 0.009 - ETA: 45512s - loss: 7.9141 - acc: 0.009 - ETA: 45343s - loss: 7.8711 - acc: 0.009 - ETA: 45210s - loss: 7.8289 - acc: 0.010 - ETA: 45079s - loss: 7.7835 - acc: 0.010 - ETA: 44944s - loss: 7.7469 - acc: 0.010 - ETA: 44809s - loss: 7.7104 - acc: 0.010 - ETA: 44701s - loss: 7.6749 - acc: 0.010 - ETA: 44581s - loss: 7.6460 - acc: 0.010 - ETA: 44476s - loss: 7.6096 - acc: 0.010 - ETA: 44404s - loss: 7.5790 - acc: 0.010 - ETA: 44333s - loss: 7.5475 - acc: 0.010 - ETA: 44228s - loss: 7.5184 - acc: 0.010 - ETA: 44139s - loss: 7.4920 - acc: 0.010 - ETA: 44042s - loss: 7.4648 - acc: 0.010 - ETA: 43945s - loss: 7.4350 - acc: 0.011 - ETA: 43855s - loss: 7.4127 - acc: 0.010 - ETA: 43787s - loss: 7.3932 - acc: 0.010 - ETA: 43729s - loss: 7.3705 - acc: 0.011 - ETA: 43677s - loss: 7.3486 - acc: 0.011 - ETA: 43604s - loss: 7.3264 - acc: 0.011 - ETA: 43528s - loss: 7.3044 - acc: 0.011 - ETA: 43453s - loss: 7.2826 - acc: 0.011 - ETA: 43383s - loss: 7.2597 - acc: 0.011 - ETA: 43310s - loss: 7.2401 - acc: 0.011 - ETA: 43226s - loss: 7.2227 - acc: 0.011 - ETA: 43159s - loss: 7.2036 - acc: 0.012 - ETA: 43122s - loss: 7.1846 - acc: 0.012 - ETA: 43079s - loss: 7.1634 - acc: 0.012 - ETA: 43016s - loss: 7.1443 - acc: 0.012 - ETA: 42958s - loss: 7.1316 - acc: 0.012 - ETA: 42891s - loss: 7.1126 - acc: 0.013 - ETA: 42824s - loss: 7.0966 - acc: 0.013 - ETA: 42761s - loss: 7.0823 - acc: 0.013 - ETA: 42704s - loss: 7.0675 - acc: 0.013 - ETA: 42650s - loss: 7.0535 - acc: 0.013 - ETA: 42607s - loss: 7.0370 - acc: 0.013 - ETA: 42573s - loss: 7.0247 - acc: 0.013 - ETA: 42531s - loss: 7.0182 - acc: 0.013 - ETA: 42476s - loss: 7.0057 - acc: 0.013 - ETA: 42427s - loss: 6.9940 - acc: 0.013 - ETA: 42378s - loss: 6.9833 - acc: 0.013 - ETA: 42327s - loss: 6.9705 - acc: 0.013 - ETA: 42277s - loss: 6.9617 - acc: 0.013 - ETA: 42223s - loss: 6.9500 - acc: 0.013 - ETA: 42172s - loss: 6.9397 - acc: 0.013 - ETA: 42130s - loss: 6.9272 - acc: 0.014 - ETA: 42084s - loss: 6.9182 - acc: 0.014 - ETA: 42048s - loss: 6.9077 - acc: 0.014 - ETA: 42004s - loss: 6.8996 - acc: 0.014 - ETA: 41952s - loss: 6.8905 - acc: 0.014 - ETA: 41900s - loss: 6.8798 - acc: 0.014 - ETA: 41846s - loss: 6.8719 - acc: 0.014 - ETA: 41797s - loss: 6.8618 - acc: 0.014 - ETA: 41749s - loss: 6.8537 - acc: 0.014 - ETA: 41702s - loss: 6.8454 - acc: 0.014 - ETA: 41661s - loss: 6.8368 - acc: 0.014 - ETA: 41625s - loss: 6.8274 - acc: 0.014 - ETA: 41596s - loss: 6.8208 - acc: 0.014 - ETA: 41555s - loss: 6.8136 - acc: 0.014 - ETA: 41506s - loss: 6.8057 - acc: 0.014 - ETA: 41461s - loss: 6.7959 - acc: 0.014 - ETA: 41410s - loss: 6.7880 - acc: 0.014 - ETA: 41368s - loss: 6.7805 - acc: 0.014 - ETA: 41330s - loss: 6.7740 - acc: 0.014 - ETA: 41288s - loss: 6.7673 - acc: 0.014 - ETA: 41252s - loss: 6.7593 - acc: 0.015 - ETA: 41223s - loss: 6.7529 - acc: 0.015 - ETA: 41181s - loss: 6.7463 - acc: 0.015 - ETA: 41142s - loss: 6.7401 - acc: 0.015 - ETA: 41103s - loss: 6.7320 - acc: 0.015 - ETA: 41058s - loss: 6.7263 - acc: 0.015 - ETA: 41014s - loss: 6.7197 - acc: 0.015 - ETA: 40973s - loss: 6.7114 - acc: 0.016 - ETA: 40932s - loss: 6.7067 - acc: 0.016 - ETA: 40902s - loss: 6.6998 - acc: 0.016 - ETA: 40880s - loss: 6.6936 - acc: 0.016 - ETA: 40850s - loss: 6.6876 - acc: 0.016 - ETA: 40809s - loss: 6.6823 - acc: 0.016 - ETA: 40769s - loss: 6.6754 - acc: 0.016 - ETA: 40726s - loss: 6.6711 - acc: 0.016 - ETA: 40687s - loss: 6.6643 - acc: 0.016 - ETA: 40641s - loss: 6.6597 - acc: 0.016 - ETA: 40599s - loss: 6.6558 - acc: 0.016 - ETA: 40573s - loss: 6.6512 - acc: 0.017 - ETA: 40547s - loss: 6.6465 - acc: 0.017 - ETA: 40519s - loss: 6.6396 - acc: 0.017 - ETA: 40476s - loss: 6.6341 - acc: 0.017 - ETA: 40443s - loss: 6.6288 - acc: 0.017 - ETA: 40422s - loss: 6.6254 - acc: 0.017 - ETA: 40381s - loss: 6.6203 - acc: 0.017 - ETA: 465720s - loss: 6.6155 - acc: 0.01 - ETA: 461918s - loss: 6.6110 - acc: 0.01 - ETA: 458173s - loss: 6.6057 - acc: 0.01 - ETA: 454484s - loss: 6.6005 - acc: 0.01 - ETA: 450849s - loss: 6.5964 - acc: 0.01 - ETA: 447262s - loss: 6.5917 - acc: 0.01 - ETA: 443743s - loss: 6.5863 - acc: 0.01 - ETA: 440275s - loss: 6.5817 - acc: 0.01 - ETA: 436876s - loss: 6.5771 - acc: 0.01 - ETA: 433516s - loss: 6.5720 - acc: 0.01 - ETA: 430209s - loss: 6.5683 - acc: 0.01 - ETA: 426952s - loss: 6.5638 - acc: 0.01 - ETA: 423763s - loss: 6.5586 - acc: 0.01 - ETA: 420642s - loss: 6.5544 - acc: 0.01 - ETA: 417552s - loss: 6.5503 - acc: 0.01 - ETA: 414473s - loss: 6.5466 - acc: 0.01 - ETA: 411441s - loss: 6.5425 - acc: 0.01 - ETA: 408451s - loss: 6.5375 - acc: 0.01 - ETA: 405500s - loss: 6.5333 - acc: 0.01 - ETA: 402603s - loss: 6.5281 - acc: 0.01 - ETA: 399744s - loss: 6.5230 - acc: 0.01 - ETA: 396923s - loss: 6.5201 - acc: 0.01 - ETA: 394144s - loss: 6.5172 - acc: 0.01 - ETA: 391426s - loss: 6.5138 - acc: 0.01 - ETA: 388735s - loss: 6.5093 - acc: 0.02 - ETA: 386074s - loss: 6.5046 - acc: 0.02 - ETA: 383443s - loss: 6.5008 - acc: 0.02 - ETA: 380849s - loss: 6.4978 - acc: 0.02 - ETA: 378305s - loss: 6.4942 - acc: 0.02 - ETA: 375815s - loss: 6.4914 - acc: 0.02 - ETA: 373329s - loss: 6.4883 - acc: 0.02 - ETA: 370884s - loss: 6.4851 - acc: 0.02 - ETA: 368457s - loss: 6.4815 - acc: 0.02 - ETA: 366077s - loss: 6.4792 - acc: 0.02 - ETA: 363752s - loss: 6.4750 - acc: 0.02 - ETA: 361423s - loss: 6.4719 - acc: 0.02 - ETA: 359124s - loss: 6.4684 - acc: 0.02 - ETA: 356843s - loss: 6.4640 - acc: 0.02 - ETA: 354617s - loss: 6.4601 - acc: 0.02 - ETA: 352395s - loss: 6.4575 - acc: 0.02 - ETA: 350204s - loss: 6.4538 - acc: 0.02 - ETA: 348034s - loss: 6.4518 - acc: 0.02 - ETA: 345894s - loss: 6.4483 - acc: 0.02 - ETA: 343787s - loss: 6.4448 - acc: 0.02 - ETA: 341755s - loss: 6.4420 - acc: 0.02 - ETA: 339722s - loss: 6.4392 - acc: 0.02 - ETA: 337714s - loss: 6.4365 - acc: 0.02 - ETA: 335733s - loss: 6.4333 - acc: 0.02 - ETA: 333775s - loss: 6.4307 - acc: 0.02 - ETA: 331837s - loss: 6.4267 - acc: 0.02 - ETA: 329921s - loss: 6.4241 - acc: 0.02 - ETA: 328033s - loss: 6.4205 - acc: 0.02 - ETA: 326176s - loss: 6.4184 - acc: 0.02 - ETA: 324310s - loss: 6.4158 - acc: 0.02 - ETA: 322473s - loss: 6.4132 - acc: 0.02 - ETA: 320640s - loss: 6.4103 - acc: 0.02 - ETA: 318814s - loss: 6.4077 - acc: 0.02 - ETA: 317008s - loss: 6.4040 - acc: 0.02 - ETA: 315227s - loss: 6.4017 - acc: 0.02 - ETA: 313470s - loss: 6.3986 - acc: 0.02 - ETA: 311726s - loss: 6.3957 - acc: 0.02 - ETA: 310009s - loss: 6.3924 - acc: 0.02 - ETA: 308311s - loss: 6.3899 - acc: 0.02 - ETA: 306644s - loss: 6.3866 - acc: 0.02 - ETA: 304982s - loss: 6.3833 - acc: 0.02 - ETA: 303343s - loss: 6.3808 - acc: 0.02 - ETA: 301717s - loss: 6.3773 - acc: 0.02 - ETA: 300117s - loss: 6.3740 - acc: 0.02 - ETA: 298542s - loss: 6.3724 - acc: 0.02 - ETA: 296967s - loss: 6.3701 - acc: 0.02 - ETA: 295416s - loss: 6.3677 - acc: 0.02 - ETA: 293871s - loss: 6.3658 - acc: 0.02 - ETA: 292340s - loss: 6.3639 - acc: 0.02 - ETA: 290824s - loss: 6.3621 - acc: 0.02 - ETA: 289328s - loss: 6.3592 - acc: 0.0252"
     ]
    }
   ],
   "source": [
    "'''\n",
    "saves the model weights after each epoch if the validation loss decreased\n",
    "'''\n",
    "checkpointer = ModelCheckpoint(filepath=path_dir+'weights.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "                        steps_per_epoch = num_train_images // batch_size,\n",
    "                        epochs = 3,\n",
    "                        validation_data = val_gen,\n",
    "                        validation_steps = num_val_images // batch_size,\n",
    "                        #workers = 8,\n",
    "                        verbose=0,\n",
    "                       callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(data_dir+'cnn_modelv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To evaluate on the validation set:\n",
    "from keras.models import load_model, Model, save_model\n",
    "model = load_model(data_dir+'cnn_modelv1.h5')\n",
    "model.evaluate_generator(val_gen, steps=num_val_images // batch_size, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Test or Submit\n",
    "\n",
    "Note that it is quite slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(data_dir + \"sample_submission.csv\")\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "data = bson.decode_file_iter(open(test_bson_path, \"rb\"))\n",
    "\n",
    "with tqdm(total=num_test_products) as pbar:\n",
    "    for c, d in enumerate(data):\n",
    "        product_id = d[\"_id\"]  \n",
    "        num_imgs = len(d[\"imgs\"])\n",
    "\n",
    "        batch_x = np.zeros((num_imgs, 180, 180, 3), dtype=K.floatx())\n",
    "\n",
    "        for i in range(num_imgs):\n",
    "            bson_img = d[\"imgs\"][i][\"picture\"]\n",
    "\n",
    "            # Load and preprocess the image.\n",
    "            img = load_img(io.BytesIO(bson_img), target_size=(180, 180))\n",
    "            x = img_to_array(img)\n",
    "            x = test_datagen.random_transform(x)\n",
    "            x = test_datagen.standardize(x)\n",
    "\n",
    "            # Add the image to the batch.\n",
    "            batch_x[i] = x\n",
    "\n",
    "        prediction = model.predict(batch_x, batch_size=num_imgs)\n",
    "        avg_pred = prediction.mean(axis=0)\n",
    "        cat_idx = np.argmax(avg_pred)\n",
    "        #print(\"avg\",cat_idx)\n",
    "        \n",
    "        submission_df.iloc[c][\"category_id\"] = idx2cat[cat_idx]        \n",
    "        pbar.update()\n",
    "        \n",
    "submission_df.to_csv(\"my_submission.csv.gz\", compression=\"gzip\", index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
